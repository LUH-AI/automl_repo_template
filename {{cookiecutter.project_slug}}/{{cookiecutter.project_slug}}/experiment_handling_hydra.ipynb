{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locating the Data\n",
    "First things first: let's check where our results are located. Use this script in combination with your command to see where the results should be if your naming matches the example config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['results/1_4/seed_0', 'results/1_4/seed_1', 'results/1_4/seed_2', 'results/1_4/seed_3', 'results/1_4/seed_4', 'results/1_4/seed_5', 'results/1_4/seed_6', 'results/1_4/seed_7', 'results/1_4/seed_8', 'results/1_4/seed_9', 'results/3_4/seed_0', 'results/3_4/seed_1', 'results/3_4/seed_2', 'results/3_4/seed_3', 'results/3_4/seed_4', 'results/3_4/seed_5', 'results/3_4/seed_6', 'results/3_4/seed_7', 'results/3_4/seed_8', 'results/3_4/seed_9', 'results/7_4/seed_0', 'results/7_4/seed_1', 'results/7_4/seed_2', 'results/7_4/seed_3', 'results/7_4/seed_4', 'results/7_4/seed_5', 'results/7_4/seed_6', 'results/7_4/seed_7', 'results/7_4/seed_8', 'results/7_4/seed_9']\n"
     ]
    }
   ],
   "source": [
    "command_str = \"python cli.py 'seed=range(0,10)' method=1,3,7\"\n",
    "seeds = range(0,10)\n",
    "methods = [1,3,7]\n",
    "benchmarks = [4]\n",
    "base_path = \"results\"\n",
    "experiment_directories = []\n",
    "\n",
    "for b in benchmarks:\n",
    "    for m in methods:\n",
    "        for s in seeds:\n",
    "            experiment_directories.append(os.path.join(base_path, f\"{m}_{b}\", f\"seed_{s}\"))\n",
    "\n",
    "print(experiment_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if Everything Ran Successfully\n",
    "Now that we know the location of our data, we can check if it's complete. For this purpose, you need to define a function that takes a directory for a single run and returns a boolean signal if this run is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_done(path_str):\n",
    "    path = os.path.join(path_str, \"done.txt\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            content = f.read()\n",
    "            if \"yes\" in content:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0/30\n",
      "Missing jobs:\n",
      "results/1_4/seed_0\n",
      "results/1_4/seed_1\n",
      "results/1_4/seed_2\n",
      "results/1_4/seed_3\n",
      "results/1_4/seed_4\n",
      "results/1_4/seed_5\n",
      "results/1_4/seed_6\n",
      "results/1_4/seed_7\n",
      "results/1_4/seed_8\n",
      "results/1_4/seed_9\n",
      "results/3_4/seed_0\n",
      "results/3_4/seed_1\n",
      "results/3_4/seed_2\n",
      "results/3_4/seed_3\n",
      "results/3_4/seed_4\n",
      "results/3_4/seed_5\n",
      "results/3_4/seed_6\n",
      "results/3_4/seed_7\n",
      "results/3_4/seed_8\n",
      "results/3_4/seed_9\n",
      "results/7_4/seed_0\n",
      "results/7_4/seed_1\n",
      "results/7_4/seed_2\n",
      "results/7_4/seed_3\n",
      "results/7_4/seed_4\n",
      "results/7_4/seed_5\n",
      "results/7_4/seed_6\n",
      "results/7_4/seed_7\n",
      "results/7_4/seed_8\n",
      "results/7_4/seed_9\n"
     ]
    }
   ],
   "source": [
    "missing = []\n",
    "for d in experiment_directories:\n",
    "    if not job_done(d):\n",
    "        missing.append(d)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print(f\"Done: {len(experiment_directories)-len(missing)}/{len(experiment_directories)}\")\n",
    "    print(\"Missing jobs:\")\n",
    "    for m in missing:\n",
    "        print(m)\n",
    "else:\n",
    "    print(\"All jobs done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Runscripts to Rerun Missing Runs\n",
    "Since it's possible some runs die before finishing, we need to rerun them at times. Here we can generate scripts to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for b in benchmarks:\n",
    "    for m in methods:\n",
    "        for s in seeds:\n",
    "            filepath = f\"missing_method_{m}_benchmark_{b}_seed_{s}.sh\"\n",
    "            command = f\"python cli.py seed={s} method={m} benchmark={b}\"\n",
    "            first = True\n",
    "            with open(filepath, \"a+\") as f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    slurm_string = f\"\"\"#!/bin/bash \\n#SBATCH --error={m}_{b}.err \\n#SBATCH --job-name=missing \\n#SBATCH --mem=10GB \\n#SBATCH --output={m}_{b}.out \\n#SBATCH --partition=ai,tnt \\n#SBATCH --time=1440 \\nconda activate my_env\"\"\"\n",
    "                    f.write(slurm_string)\n",
    "                    f.write(\"\\n\")\n",
    "                f.write(command)\n",
    "                f.write(\"\\n\")\n",
    "            all_files.append(filepath)\n",
    "\n",
    "with open(\"submit_all_missing.sh\", \"a+\") as f:\n",
    "    for file in all_files:\n",
    "        f.write(f\"sbatch {file}\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# now we could 'sbatch submit_all_missing.sh' in the terminal to run the missing jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "There are multiple ways to then work with your data. We'll look at getting all of it into a single DataFrame here - if you need to load things differently, you'll have to edit the code. You can either use these dataframes directly for plotting or, even better, save one .csv per experiment. This will make rerunning and data loading much easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for d in experiment_directories:\n",
    "    path = os.path.join(d, \"performance.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Add sweep parameters\n",
    "    seed = d.split(\"_\")[-1][-1]\n",
    "    df[\"seed\"] = [seed] * len(df)\n",
    "    method = d.split(\"_\")[-2]\n",
    "    df[\"method\"] = [method] * len(df)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to single csv file\n",
    "data.to_csv(\"all_performances.csv\")\n",
    "\n",
    "# split into separate files by method\n",
    "for m in df[\"method\"].unique():\n",
    "    df[df[\"method\"] == m].to_csv(f\"{m}_performances.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_deepcave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
